# -*- coding: utf-8 -*-
"""ai-project-ipynb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MOFwUloJtwKITTnV65Acqi2nTTtC30PC

# **Drowsiness Detection System**

**Importing Required Libraries**
"""

import numpy as np
import pandas as pd
import tensorflow as tf
tf.random.set_seed(42)
import matplotlib.pyplot as plt
from glob import glob
import cv2
import os
import shutil
import random
import math
import tensorflow_datasets as tfds

"""**Checking TensorFlow Version**"""

from tensorflow.python import keras
print(keras.__version__)

"""**Creating a Folder to Store TFRecords**"""

!mkdir images

"""**Function to Convert Arrays to Binary Stream**"""

def _bytes_feature(value):
    if isinstance(value, type(tf.constant(0))): 
        value = value.numpy() 
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

"""**Function to Convert A Sample to Binary Stream**"""

def serialize_example(image,label):
    feature = {
        'image':tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),
        'label':_bytes_feature(tf.io.serialize_tensor(label))
    }
    example = tf.train.Example(features=tf.train.Features(feature=feature))
    return example.SerializeToString()

"""**Function to Create TF Records from Binary Stream Objects**"""

def create_records(path,op_path='./images/tensor_records'):
    classes=os.listdir(path)
    with tf.io.TFRecordWriter(op_path) as writer:
        files_list = glob(path+'/*/*')
        random.shuffle(files_list)
        for fn in files_list:
            label = np.array([0,0],dtype='float64')
            img = open(fn,'rb').read()
            lab = fn.split('/')[-2]
            lab = classes.index(lab)
            label[lab] = 1.0
            tf_example = serialize_example(img,label)
            writer.write(tf_example)

"""**Calling all the above Functions**"""

create_records("../input/mrl-dataset/train")

"""**Below Functions create TF Dataset from TFRecords effeciently**"""

def parse_image(example):

    feature = {'image':tf.io.FixedLenFeature([],tf.string),
              'label':tf.io.FixedLenFeature([],tf.string)}
    features = tf.io.parse_single_example(example,feature)
    image = tf.io.decode_jpeg(features['image'],channels = 3)
    image = tf.image.resize(image,[256,256])
    label = tf.io.parse_tensor(features['label'], out_type=tf.float64)
    label = tf.reshape(label,shape=(2,))
    return image,label
def read_dataset(filename,batchs):
    data = tf.data.TFRecordDataset(filename)
    img_count = len(glob("../input/mrl-dataset/train" +'/*/*'))
    val_size = int(img_count * 0.2)
    train_ds = data.skip(val_size)
    val_ds = data.take(val_size)
    train_ds = configure(train_ds)
    val_ds =  configure(val_ds)
    return train_ds,val_ds
    
def configure(data):
    data = data.map(parse_image,num_parallel_calls = tf.data.experimental.AUTOTUNE)
    data = data.shuffle(500)
    data = data.batch(32,drop_remainder = True)
    data = data.repeat()
    data = data.prefetch(buffer_size = tf.data.experimental.AUTOTUNE)
    return data

"""**Calling the above functions to Create a full TF dataset**"""

def use_records():
    train,val = read_dataset("./images/tensor_records",32)
    return train,val
train_ds,val_ds = use_records()

"""**Checking the shape of a batch in our TF Dataset**"""

for image, label in val_ds.take(1):
  print("Image shape: ", image.numpy().shape)
  print("Label: ", label.numpy().shape)

"""**Visualising a Sample**"""

#Label 1 , 0 => Open eye & Label 0 , 1 => Closed eye
for image, label in val_ds.take(1):
  print("Label: ", label.numpy()[0])
  plt.imshow(image.numpy()[0].astype("uint8"))

"""**Lenght of Training Data and Validation Data**"""

img_count = len(glob("../input/mrl-dataset/train" +'/*/*'))
val_size = int(img_count * 0.2)
print(img_count-val_size,val_size)

"""**Creating CallBacks to choose the choose the epoch's weight that gives the best score**"""

cb = [tf.keras.callbacks.ModelCheckpoint("./first_model.h5",monitor='val_accuracy',save_best_only=True,mode='max')]

"""**Building CNN Architecture**"""

def build():
  model = tf.keras.Sequential([
  tf.keras.layers.Resizing(256,256),
  tf.keras.layers.Rescaling(1./255),
  tf.keras.layers.RandomFlip(mode="horizontal"),
  tf.keras.layers.RandomRotation(0.2, fill_mode='reflect',interpolation='bilinear'),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Conv2D(32, 3, activation='relu'),
  tf.keras.layers.MaxPooling2D(),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dense(2,activation='softmax')
 ])
  model.compile(
  optimizer='adam',
  loss=tf.losses.BinaryCrossentropy(),
  metrics=['accuracy'])
  return model

"""**Calling CNN Architecture Function**"""

model = build()

"""**Training our own scratch-built CNN**"""

deep_model = model.fit(train_ds,steps_per_epoch=math.ceil(3200/32),validation_data=val_ds,validation_steps = math.ceil(800/32),epochs =10,callbacks=cb)

"""**Visualsing Our Model's Performance**"""

pd.DataFrame(deep_model.history).plot()

"""**Importing the best model to Test Our Model**"""

loaded_model = tf.keras.models.load_model("./first_model.h5")

"""**Evaluating Our Imported Model**"""

loaded_model.evaluate(val_ds,steps = math.ceil(800/32))

"""**Summary of our scratch-built CNN**"""

loaded_model.summary()